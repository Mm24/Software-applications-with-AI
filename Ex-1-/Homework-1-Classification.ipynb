{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Challenge\n",
    "\n",
    "For a given set of financial transactions, classify each one into one of seven revenue (transaction) categories. \n",
    "In this challenge, a \"Multinomial Naive Bayes\" classifier is trained  and fitted with the trasaction  word counts and class categori  values. The categories are\n",
    "- Income\n",
    "- Private (cash, deposit, donation, presents)\n",
    "- Living (rent, additional flat expenses, ...)\n",
    "- Standard of living (food, health, children, ...)\n",
    "- Finance (credit, bank costs, insurances, savings)\n",
    "- Traffic (public transport, gas stations, bike, car rent, ...)\n",
    "- Leisure (hobby, sport, vacation, shopping, ...) \n",
    "\n",
    "\n",
    "## Aproach \n",
    "Implement a Generic Naive Bayes Classifier\n",
    "\n",
    "1. Clean and prepare the given data\n",
    "2. Label the data and store it\n",
    "3. Define the features you want to use\n",
    "4. Prepare your features / transform them into a format you can work with\n",
    "5. Train your model\n",
    "5. Evaluate your model\n",
    "6. Visualize your results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Load necessary libraries for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Imports for Classification \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, f_classif\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the data in a pandas Dataframe structure \n",
    "and have an some information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data as pandas Dataframe structure\n",
    "data = pd.read_csv(\"SAKI Exercise 1 - Transaction Classification - Data Set.csv\",sep=';',index_col=0, encoding='utf8',header=0)\n",
    "\n",
    "# Print summary of the column names and Data types \n",
    "print(' Columns  \\t \\t \\t Data Types\\n' + 20*'---'+'\\n', data.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration\n",
    "\n",
    "In order to obtain a better understanding of data and valuable features, somo basiuc exploration is done  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Print an first row of the data as overview\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(' Columns  \\t \\t \\t Unique Values \\t \\t \\t Empty Values \\n' + 40*'--')\n",
    "for col in data.columns: \n",
    "    print(\"{0: <35} {1: <35} {2: <35}\".format(col, len(data[col].unique()), data[col].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the Data class frequencies to see if there is a class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Print summary of the classes and lablel numbers\n",
    "print('\\nClass Names \\t  Class Frequencies\\n' + 20*'--')\n",
    "for class_label, c in zip( data.label,data.label.value_counts()) : \n",
    "    print(\"{0: <20} {1: <20}\".format(class_label, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary functions to standarize text columns and format. Remove invalid characters in order to standarize and avoid mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text_fields(text, remove_stopwords=True):\n",
    "    #remove punctuation \n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "    #convert text to lower case\n",
    "    text = text.lower()\n",
    "    return textre\n",
    "\n",
    "# def remove_special_characters_and_numbers(str):\n",
    "#     str = re.sub('[\\W_]+', '', str)\n",
    "#     str = re.sub('[\\d_]+', '', str)\n",
    "#     return str;\n",
    "\n",
    "def remove_special_characters(df_column): \n",
    "    df_column = df_column.str.lower() # Just for assurance\n",
    "    df_column = df_column.str.replace('[^a-z-]', ' ')\n",
    "    return df_column\n",
    "\n",
    "\n",
    "def tokenize(df_column, language = 'german'):\n",
    "    stopwords = stopwords.words(language)\n",
    "    df_column = df_column.str.replace('\\s+', ' ', regex=True)\n",
    "    tokens  = df_column.apply(lambda l: ' '.join([word for word in l.split() if (word not in (stopwords) and len(word) > 2)]))\n",
    "    return tokens\n",
    "\n",
    "def tokenize_text(text, language = 'german'): \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for t in tokens:\n",
    "        if t in stopwords.words(language):\n",
    "            tokens.remove(t)\n",
    "    #Join the tokens\n",
    "    return  \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Remove the columns that have not many different values and threfore are probablyu mot meaningful√± for classification and could lead to overfit problem\n",
    "non_meaningul_columns = [\"Auftragskonto\",  \"Valutadatum\", \"Kontonummer\", \"BLZ\", \"Waehrung\"]\n",
    "data = data.drop(columns=non_meaningul_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Clean and prepare the given data casting to proper Data Type\n",
    "\n",
    "# Remove the invaled values with a numeric(-11111)\n",
    "data = data.fillna(-11111)\n",
    "\n",
    "#Remove the punctuation and making all the words lowercase for comparison\n",
    "data['Betrag'].replace(regex=True, inplace=True, to_replace=',', value='.')\n",
    "data['Betrag'] = data['Betrag'].astype('float')\n",
    "\n",
    "#Convert to String  Datatypes\n",
    "# data['Kontonummer'] = data['Kontonummer'].astype('str')\n",
    "# data['BLZ'] = data['BLZ'].astype('str')\n",
    "#Convert to Date Time format\n",
    "#data['Buchungstag'] = data.to_datetime(df['Buchungstag'])\n",
    "\n",
    "#Replace the text fields to lower char values\n",
    "data['Buchungstext'] = data['Buchungstext'].str.lower()\n",
    "data['Beguenstigter/Zahlungspflichtiger'] = data['Beguenstigter/Zahlungspflichtiger'].str.lower()\n",
    "\n",
    "# Store the Column Index\n",
    "column_index = data.columns\n",
    "\n",
    "# Select columns that are not necessary ( no causality) to the labels by removing them from the data\n",
    "#delete_column_ind = ['Auftragskonto', 'Buchungstag', 'Valutadatum', 'Kontonummer', 'BLZ', 'Waehrung']\n",
    "#data.drop(data[delete_column_ind], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fearures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}